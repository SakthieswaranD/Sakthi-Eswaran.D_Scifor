{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d721d9cd-2fed-42fb-9880-63785d2ed645",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1How many types of modes we have in spark?\n",
    "#Modes-- FailFast,Dropmal,Permissive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "251a0705-3093-4bed-961e-46a723136900",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2.What is cluster in Spark?\n",
    "#A cluster is which one has driver and worker nodes to do tasks in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c13eff8-24e1-4719-b9af-c1825c96a3b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3what is a table in spark?\n",
    "#A table in spark is one which has rows and columns and every rows has null or nonnull values along the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e67a7ca-12f6-47db-bae3-d37c351343ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3825972520508344#setting/sparkui/0229-125944-j3fjzc0v/driver-270491605990209818\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3825972520508344#setting/sparkui/0229-125944-j3fjzc0v/driver-270491605990209818\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#4Read the dataset in databricks community\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a59892f0-0a6a-4a3d-9deb-bd1bdaeddc86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").option('mode','FAILFAST').load(\"dbfs:/FileStore/shared_uploads/sakthieswarandp@gmail.com/world_population_data1-1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65272a5c-286a-468a-a641-186f05cfe311",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5What would you do if you want to show the header while showing up 5 records in table? write the code\n",
    "#when creating the dataframe for csv file we have give header option true for showing the header then to show 5 records just have use show function\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").option('mode','FAILFAST').load(\"dbfs:/FileStore/shared_uploads/sakthieswarandp@gmail.com/world_population_data1-1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e393ec97-cf31-436e-bee6-05fa0f4e1e85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------------+-------------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------+--------------+-----------+----------------+\n|rank|cca3|      country|    continent|      2023|      2022|      2020|      2015|      2010|      2000|      1990|     1980|     1970|area(sqkm)|density_(sqkm)|growth_rate|world_percentage|\n+----+----+-------------+-------------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------+--------------+-----------+----------------+\n|   1| IND|        India|         Asia|1428627663|1417173173|1396387127|1322866505|1240613620|1059633675| 870452165|696828385|557501301| 3287590.0|           481|      0.81%|          17.85%|\n|   2| CHN|        China|         Asia|1425671352|1425887337|1424929781|1393715448|1348191368|1264099069|1153704252|982372466|822534450| 9706961.0|           151|     -0.02%|          17.81%|\n|   3| USA|United States|North America| 339996563| 338289857| 335942003| 324607776| 311182845| 282398554| 248083732|223140018|200328340| 9372610.0|            37|      0.50%|           4.25%|\n|   4| IDN|    Indonesia|         Asia| 277534122| 275501339| 271857970| 259091970| 244016173| 214072421| 182159874|148177096|115228394| 1904569.0|           148|      0.74%|           3.47%|\n|   5| PAK|     Pakistan|         Asia| 240485658| 235824862| 227196741| 210969298| 194454498| 154369924| 115414069| 80624057| 59290872|  881912.0|           312|      1.98%|           3.00%|\n+----+----+-------------+-------------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------+--------------+-----------+----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b479195-e9bb-43d9-8a14-39e9730289c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: 234"
     ]
    }
   ],
   "source": [
    "#6.What is count?Perform in spark\n",
    "#Count gives the no of records in dataframe when used along column it will give the total no of values in the column.\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1b164c-d8b0-47e8-90c8-7871a4a71469",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n|    continent|count|\n+-------------+-----+\n|       Europe|   50|\n|       Africa|   57|\n|North America|   40|\n|South America|   14|\n|      Oceania|   23|\n|         Asia|   50|\n+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#7.What is groupby? perform in spark.\n",
    "#Groupby is transformation which is used to make  group along a particular column in a dataframe with some aggregate function(s). \n",
    "df1.groupby('continent').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5661c33-dd3b-4fba-b470-e3037f36feb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Test 2024-02-29",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
